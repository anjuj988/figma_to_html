{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PP-StructureV2**  \n",
    "PP-StructureV2 is an intelligent document analysis system designed to handle layout analysis, table recognition, and key information extraction in image/PDF documents. It enables structured document understanding and restoration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow**\n",
    "\n",
    "Image Correction → Determines document orientation and corrects distortions.  \n",
    "Layout Analysis → Identifies text, tables, images, and formulas within the document.  \n",
    "Text & Table Processing:\n",
    "   OCR Engine → Extracts and recognizes text.  \n",
    "   Table Recognition → Converts detected tables into structured formats (e.g., Excel).  \n",
    "Layout Recovery → Restores extracted information into a Word/PDF format while maintaining original structure.    \n",
    "Key Information Extraction (KIE):  \n",
    "  Semantic Entity Recognition (SER) → Identifies key entities in the text.  \n",
    "  Relation Extraction (RE) → Establishes relationships between extracted entities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For random, non-tabular bill images:**\n",
    "\n",
    "PaddleOCR's core text detection and recognition will extract all visible text.  \n",
    "Post-processing with regex or NER will allow you to identify and extract specific fields like totals, dates, etc.   \n",
    "PP-Structure might not be directly useful unless you need very structured, table-like data.   But for bills, focusing on OCR + field extraction techniques will give you more control over diverse layouts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Layout Information Extraction **   \n",
    "**Problem:** Extracted text is spread across different lines, making it difficult to associate the correct values with labels (e.g., \"invoice number\" being on one line, and its value on another).  \n",
    "**Solution:** PP-StructureV2 includes Layout Information Extraction and Layout Restoration modules, which can help reconstruct the layout of the document more accurately. This would allow the system to better understand the relationship between the labels (e.g., \"invoice number\") and their corresponding values, even if they are on different lines.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the PP-StructureV2 model\n",
    "```python\n",
    "self.ppstructure_config = {\n",
    "    'use_gpu': False,             # Set to False for CPU, or True for GPU\n",
    "    'layout': True,               # Enable layout analysis (tables, blocks, etc.)\n",
    "    'table': True,                # Enable table structure recognition\n",
    "    'rec': True,                  # Enable text recognition within detected structures\n",
    "    'formula': False,             # Disable formula recognition (set to True if needed)\n",
    "    'lang': 'en',                 # Set language for OCR (e.g., English)\n",
    "    'rec_image_shape': '3, 48, 320', # Input image shape for text recognition (optional)\n",
    "    'drop_score': 0.5,            # Confidence threshold for structure detection\n",
    "    'det_db_thresh': 0.3,         # Threshold for text detection (DB algorithm)\n",
    "    'det_db_box_thresh': 0.6,     # Minimum threshold for bounding box confidence\n",
    "    'det_db_unclip_ratio': 1.5,   # Unclipping ratio for box expansion\n",
    "    'rec_batch_num': 6,           # Batch size for text recognition\n",
    "    'max_text_length': 25,        # Maximum text length in the extracted cells\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Notes:**  \n",
    "PPStructure does not directly use the rec_algorithm parameter. The text recognition model for PPStructure is managed internally.  \n",
    "The det_algorithm for layout detection (table and block detection) is handled by PPStructure automatically.  \n",
    "rec_image_shape, det_db_thresh, and other OCR-specific parameters can be configured if necessary, but are not strictly required for PPStructure unless you are fine-tuning performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Changes for PPStructure: \n",
    "\n",
    "Added for PPStructure:  \n",
    "\n",
    "layout: Enables layout analysis.  \n",
    "table: Enables table detection.  \n",
    "formula: Enables formula detection.  \n",
    "rec: Enables text recognition within detected structures.\n",
    "\n",
    "Not Needed for PPStructure:  \n",
    "\n",
    "rec_algorithm: Handled internally in PPStructure.  \n",
    "det_algorithm: PPStructure uses its own layout detection algorithm (DB).  \n",
    "cls_batch_num & cls_thresh: Not needed for PPStructure.  \n",
    "det_limit_side_len & det_limit_type: Not needed for PPStructure.  \n",
    "max_text_length: Not required unless fine-tuning text extraction for specific use cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When rec=True**, the internal recognition will process the detected regions and extract any text from the detected structures (e.g., text inside a table, headings, or text blocks). This happens through the internal OCR model, which is one of the methods I described above.   \n",
    "\n",
    "You do not need to specify the recognition algorithm explicitly when using PPStructure because it’s already integrated into the model. However, you can specify or tweak the recognition model and detection models in PaddleOCR settings if you want to customize the OCR system further.  \n",
    "\n",
    "Summary:  \n",
    "The internal recognition algorithm used **when rec=True is typically PaddleOCR's OCR model, which may include CRNN or SVTR-LCNet depending on the task.**  \n",
    "You don’t need to manually specify the recognition model, as PPStructure handles this internally when you enable rec=True.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "```python\n",
    "from paddleocr import PPStructure, draw_structure_result, save_structure_res\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Set PPStructure for table detection with CPU\n",
    "table_engine = PPStructure(\n",
    "    use_gpu=False,  # Use CPU\n",
    "    layout=True,    # Enable layout analysis\n",
    "    table=True,     # Enable table detection\n",
    "    rec=True,       # Enable text recognition\n",
    "    formula=False   # Disable formula recognition\n",
    ")\n",
    "\n",
    "# Provide the path to your image\n",
    "img_path = \"/content/food-bill_10.png\"\n",
    "img = cv2.imread(img_path)  # Read image using OpenCV\n",
    "\n",
    "# Process the image using PPStructure\n",
    "result = table_engine(img)  # Correct way to pass image\n",
    "\n",
    "# Print structured OCR results\n",
    "for line in result:\n",
    "    print(line)\n",
    "\n",
    "# Save or visualize results if needed\n",
    "save_structure_res(result, \"/content/output\", os.path.basename(img_path))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature            | PaddleOCR                         | PP-Structure                        |\n",
    "|--------------------|---------------------------------|-----------------------------------|\n",
    "| **Purpose**       | Focuses on text detection and recognition in images. | Aims at document analysis, including layout detection, table recognition, and key information extraction. |\n",
    "| **Main Components** | - Text detection (DB, EAST, etc.)<br>- Text recognition (CRNN, SVTR, etc.)<br>- Text direction classification | - Layout analysis (detecting text, tables, images, etc.)<br>- Table recognition (detecting and parsing table structures)<br>- Key information extraction (KIE) |\n",
    "| **Input Type**    | Single images containing text | Documents, scanned pages, forms, tables, and receipts |\n",
    "| **Output**        | Recognized text with bounding boxes | Structured document data (JSON/XML format with text, tables, and layout info) |\n",
    "| **Use Cases**     | OCR applications like license plate recognition, scene text recognition, and handwritten text detection | Document digitization, invoice processing, form extraction, and table structure recognition |\n",
    "| **Dependencies**  | Uses PaddlePaddle deep learning framework | Built on top of PaddleOCR with additional modules for document layout analysis |\n",
    "| **Pretrained Models** | Offers text detection and recognition models | Includes models for layout parsing, table structure recognition, and key-value extraction |\n",
    "| **Customization** | Supports fine-tuning OCR models | Supports fine-tuning for document structure analysis |\n",
    "| **Example Output** | `\"Detected Text: 'Invoice 1234'\"` | `{ \"layout\": \"table\", \"text\": { \"header\": \"Invoice\", \"row1\": [\"1234\", \"Item A\", \"$10\"] } }` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output structure:\n",
    "\n",
    "1. 'type': 'text'  \n",
    "This indicates that the detected structure is a text block. It could be part of a table, a standalone piece of text, or other textual content within the image.  \n",
    "2. 'bbox': [417, 114, 578, 220]  \n",
    "'bbox' stands for bounding box, and it describes the position of the detected text in the image.  \n",
    "The four numbers are:  \n",
    "**[417, 114]: The top-left** corner of the bounding box (x, y coordinates).  \n",
    "**[578, 220]: The bottom-right** corner of the bounding box (x, y coordinates).  \n",
    "These values are pixel coordinates, meaning the detected text starts at (417, 114) and ends at (578, 220) in the image.  \n",
    "\n",
    "3. 'img': array([...])  \n",
    "The 'img' key contains a cropped image of the detected text or structure within the bounding box.  \n",
    "The array([...]) represents the pixel values of the region within the bounding box, essentially a small part of the original image containing just the text.  \n",
    "In this case, the pixel values are in the range of RGB values (e.g., [255, 255, 255] representing white pixels, etc.).  \n",
    "Summary:  \n",
    "Bounding Box ('bbox'): Describes the area of the image where the text or structure is located, using the top-left and bottom-right pixel coordinates.  \n",
    "Image ('img'): Contains the cropped image of the detected structure (in this case, text), so you could extract and process it further if necessary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
